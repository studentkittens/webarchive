Komponenten:
------------
* Crawler (1 bis n Instanzen)
    -Arbeitet von einem (oder mehreren) Startknoten und arbeitet alle Links ab
    -Seitensprache konfigurierbar, als Liste
        Methoden zur Spracherkennung einhängbar (z.B. Attribut in Header, Keywords)
    -Weitere Filter (pre) als Plugin einhängbar, zum Entscheiden ob Seite akzeptiert
        (für Wikipedia -> Domainfilter)
    -vor dem Speichern, mehrere parallele unabhängige Analysen

* AnalyseMethoden
    -Speicherung von Metadaten in Dateien
    ???-Normalisierung von Zeichenketten?

* CrawlerManager
    -Frequenz der Suche konfigurierbar
    -Suchtiefe konfigurierbar
    -Dictionary mit allen aktiv berarbeiteten Seiten pro Zyklus
* Ingestion (DB Backend)
    -Test auf Duplikate 
    -Test auf Änderungen
    ???-soll konfigurierbar sein, so dass auch Metadaten lesen kann, die von zukünftigen Analysetools erzeugt werden
Metadaten:
    -URI
    -TimeStamp "Akquisition"
    -TimeStamp "Letzte Änderung" oder Checksum
    -Title
    -Language 
    (-Autor)

* FileManager
    -Komprimierung (? jedes File einzeln oder gesamtes System oder Strukturen)



Datenmodell

Ablauf
CrawlerMangager erstellt Crawlerthreads mit Startseite usw.
Wenn Seite im Dictionary als "Besucht" eingetragen worden ist: Abbruch
Crawler parsed alle Links auf der Seite
Rekursiver Aufruf der neuen Seiten, neue Threads bis Obergrenze

stand 04.04.2012
crawl-vorgang
- mehrere wget-instanzen crallen domain (parameter über ui einstellbar)
- je wget ein tmp ordner
- optional tmp ordner ins ramfs (tempfs) laden
- html dateien in gleichnamigen ordner verschieben (ordner vorher erzeugen)
	der ordner wird dann später als archiv benutzt --> komprimierung teilweise über git
- xml-metadaten (parallel) auf tmp-ordner erzeugen, evtl. ordner bereinigen
- tmp-ordner ins archiv syncen (rsync), dabei wird über domain-ordner gelockt (fs-mutex).
	--> sonst warteschlange, mit anderem ordner weitermachen
- tmp-ordner löschen
- archiv wird über git versioniert, commit dann nach aktualisierung, log file für db aktualisierung erstellen
	(achtung commit hash in db schreiben, besser taggen, wg. xml-metadatei)
- liste für db-aktualisierung erzeugen, db updaten (sqlite)
--> Analyse starten

wiederherstellung db
- db anlegen (datei)
- tabellen anlegen
- archiv ordnerstruktur rekursiv durchsuchen und xml-metadaten in die db schreiben

analyse
- Schnittstelle (interface) definieren, (Analyseobjekte werden beim Start statisch eingehängt)
- optional: Socket / TCP/IP - Verbindung, Textbasierte commands verschicken. 
  Beispielsweise:
    - listfiles
    - query <argumente>
    - idle (lässt die verbindung solange schlafen bis events auftreten (neue files zB.))
- alternativ: HTTP Webserver, abfrage durch ein webservice (zB: http://webarchive.fh-hof/api/crawl?depth=5&domain=heise.de

ui 
- webarchivserver, kann textbasierte anfragen erhalten
	-Abfrage einzelne HTML-Files, Rückgabe Archiv des HTMLS mit Metadaten und Analysefiles
	-Abfrage Domain, Rückgabe von HTML-Fileliste
-optional: Analysetools registrieren

config
-alle parameter,
-URI zum crallen




- kommandenzeilen client

