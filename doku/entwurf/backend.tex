\chapter{Backend}
\label{cha:backend}
% chapter backend (end)
Das Backend bezieht sich auf den in Python geschriebenen Teil, welcher öffentlich nicht sichtbar ist.

\section{Zentrale Module} 
\label{sec:zentrale_module}

\subsection{ConfigHandler}
\label{sub:confighandler}
Dieser wird zum Lesen und Setzen der Konfigurationswerte verwendet. Die Konfigurationsdatei ist in 
Xml geschreiben und hat folgenden Aufbau:

\lstinputlisting[language=XML,basicstyle=\ttfamily\fontsize{8}{10}\selectfont]{../../conf/webarchive.conf.xml}

% subsection confighandler (end)
\subsection{Commandline Interface}
\label{sub:commandline_interface}
Dieses Interface realisiert die zentralle Administrationsschnittstelle zum Webarchiv. 
Das Interface soll dabei ähnlich wie git nach dem folgenden Schema funktionieren:
\begin{verbatim}
archive [--general-options] submodule [--arguments-specific-to-submodule]
\end{verbatim}
submodule kann dabei einer der folgenden Module sein:
\begin{enumerate}
    \item init
    \item crawler
    \item javadapter 
    \item db
    \item config
\end{enumerate}

\subsection{Initialisierung} 
\label{sub:initialisierung}
Durch dieses Modul wird ein leerer Archivordner erstellt, der lediglich
ein Default-Configtemplate mit dem Pfad zum Archiv enthält, sowie einen leeren Ordner ,,content'',
in dem später die Crawldaten abgelegt werden.
Weitere Einstellung müssen händisch in der Config nachgetragen werden.
\\
Angelegt wird der Ordner über %\verbatim{archive init /pfad/zum/archive}
% subsection inititialisierung (end)

\subsection{Crawlermanager}
\label{sub:crawlermanager}
Der Crawlermanager liest eine Liste mit URLs aus einer Datei, die entweder auf der Kommandozeile oder in
der Config definiert wurde. Die URLs sind in diese Datei zeilenweise einzutragen und können durch ein \# auskommentiert werden.
Über einen ThreadPool wird dann für jede URL ein Crawljob gestartet. Die maximale Anzahl der dabei laufenden Crawljobs wird bei
der Instanzierung des ThreadPools aus der Config gelesen.
\\
Desweiteren hat der Crawlermanager nur administrative Funktionen wie dem Stoppen der laufenden Crawljobs.
% subsection crawlermanager (end)

\subsection{Crawljob}
\label{sub:crawljob}
Der Crawljob ist ein autonomer Thread der die unten aufgelisteten Submodule beeinhaltet.
Bevor die einzelnen Submodule abgearbeitet werden, wird ein temporäres Arbeitsverzeichnis angelegt.
Die Lage dieses Verzeichnisses kann in der Confug festgelegt werden.

\subsubsection{Wget}
\label{ssub:wget}
Eine Managementschicht zur Abstraktion von wget.
Dieser wird eine Domain zugeteilt welche von wget mit konfigurierbaren Parametern gecrawlt wird.
Die allgemeine Webseitenstruktur wird dabei intakt gelassen:

\begin{verbatim}
www.heise.de/
    index.html
    news/
        newsfeeds.html
        favicon.png
        ...
\end{verbatim}


\subsubsection{Cleaner}
\label{ssub:cleaner}
Der Cleaner bereingt die Verzeichnisstruktur indem er leere Ordner und Dateien löscht.
Desweiteren wird folgende Restrukturierung durchgeführt:
\begin{verbatim}
www.heise.de/ 
    index.html/
        data
    news/
        newsfeeds.html/
            data
        favicon.png/
            data
        ...
\end{verbatim}


\begin{itemize}
    \item Ordner werden intakt gelassen
    \item Für jede reguläre Datei wird ein gleichnamiges Verzeichnis angelegt
    \item Die regulären Daten werden in dieses Verzeichnis mit den Namen ,,data'' verschoben.
\end{itemize}

Außerdem werden während des Strukturierens die grundlegenden Metadatan als Liste im Speicher gesammelt.
Bevor die Datei an das Filtersystem weitergeleitet wird, wird ein ,,Titel'' abhängig vom MIME-Type ermittelt.
Pro Datei werden die konfigurierten Filter ausgeführt, welche entscheiden ob eine Datei gelöscht werden soll oder nicht.
Gibt ein Filter ,,false'' zurück, so wird der jeweilige Dateiordner samt Inhalt sowie die Metadatan gelöscht,
ohne dass weitere Filter aufgerufen werden müssen.
% subsubsection Cleaner (end)

\subsubsection{Filtersystem}
\label{ssub:filtersystem}
Das Filtersystem entscheidet aufgrund eines übergebenen Metadaten-Dictionaries ob eine Datei behalten wird. Exemplarisch wird ein HTML-Filter
implementiert. Die Filter werden jeweils in einem Subintepreter gestartet, wobei die Metadaten im Subintepreter global vorhanden sind.
% subsubsection filtersystem (end)


\subsubsection{XmlGen}
\label{ssub:xmlgen}
Hierbei werden aus der Metadatenliste Xml-Dateien zum jeweiligen Content geschrieben (data.xml). 
Beim Xml generieren werden die Metadaten mit dem aktuellen Systemdatum versehen.
% subsubsection xmlgen (end)


\subsubsection{DBGen}
\label{ssub:dbgen}
Analog zur Xmlgenerierung wird ein SQL-Statement erstellt, dass Daten aktualisiert oder neu hinzufügt.
Ist die Datenbank noch nicht vorhanden so wird sie neu erstellt.
% subsubsection dbgen (end)


\subsubsection{Rsync}
\label{ssub:rsync}
Rsync ist eine Managementschicht für das Unix Tool ,,rsync''. Rsync wird verwendet um die gecrawlten Daten sauber ins Archiv zu synchronisieren.
Hierbei wird der aktuelle gecrawlte Inhalt ins Archiv gespiegelt. 
% subsubsection rsync (end)


\subsubsection{Git}
\label{ssub:git}
Das Git Modul ist ein Wrapper für das SCM ,,git''. Es wird verwendet um das Archiv mit einer Versionierung auszustatten. Hierzu werden 
die Dateien ins Archiv per Git hinzugefügt, der Stand wird ,,commited'' und ein CommitTag generiert.
% subsubsection git (end)


% subsection crawljob (end)

\subsection{Logger}
\label{sub:logger}
Der Logger wird für das zentrale Loggen im Backend verwendet. Er implementiert die folgenden Error Level:
\begin{itemize}
    \item DEBUG
    \item CRITICAL
    \item ERROR
    \item WARNGING
    \item INFO
\end{itemize}

Folgende Funktionen werden bereitgestellt:
\begin{verbatim}
log.print(severity, *messages)
log.verbosity(SEVERITY)
\end{verbatim}
% subsection logger (end)

% subsubsection  (end)

\section{Util} 
\label{sec:util}
Das Util Modul stellt verscheidene Funktionen die von mehreren Modulen genutzt werden bereit. Erwähnenswert ist hier der ,,Lockmechanismus''
auf Dateisystemebene um Daten beim Zugriff auf gemeinsame  Ressourcen zu schützen.

% section util (end)


\section{Javadapter} 
\label{sec:javadapter}
Der ,,Javadapter'' stellt eine Schnittstelle zwischen Python und Java dar. Er ist als Daemon implementiert der über einen Socket über einen definierten Port
erreichbar ist. Interagiert werden kann mit dem Adapter über ein Textprotokoll. Der Client (in unserem Fall der Java-Server) kann bestimmte Kommandos
zeilenweise an den Server schicken, welcher dann mit ,,OK'' oder ,,ACK [Fehler]' antwortet.
\\
Folgende Kommandos sollen dabei implementiert werden:
\begin{itemize}
    \item \texttt{lock [domain]}
        Lockt eine bestimmte Domain mittels einen Lockfiles
    \item \texttt{unlock [domain]}
        Löscht ein durch \texttt{lock} erzeugtes Lockfile
\end{itemize}
% section javadapter (end)
