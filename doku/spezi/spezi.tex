\chapter{Beschreibung des Programmablaufs}
Über einen konfigurierbaren Crawler können HTML-Inhalte von Webseiten bis zu einer bestimmten Tiefe
aus dem Netz in ein Archiv geladen werden. Da dies parallelisiert erfolgen soll,
müssen die Daten nach dem Herunterladen von temporären Verzeichnissen in das gemeinsame
Archivverzeichnis synchronisiert werden. Der aus der URI extrahierte Pfad der HTML-Dateien wird
dabei auf das Archiv abgebildet, wobei jede HTML-Datei in einen eigenen Archivordner verschoben wird. \\
Beim Crawlvorgang werden zusätzlich Metadaten der HTML-Seiten erstellt. 
Diese werden in einer Datenbank und als XML-Datei im jeweiligen HTML-Ordner gespeichert.
Die Datenbank soll dabei wieder aus den XML-daten rekonstruierbar sein. \\
Außerdem sollen Filter eingehängt werden können, die bereits in den TMP-Ordnern ungültige Dateien löscht.
Über eine Java-Schnittstelle kann anschließend wieder auf die Daten zugegriffen werden.
Dabei sollen neben dem Auslesen von vorhandenen Dateien auch neue Dateien den Archivordnern hinzugefügt werden können.
Ebenso sollen die oben genannten XML-Daten um neue Nodes erweiterbar sein.
Zu Vorführzwecken wird ein Testanalysetool erstellt, welches die bereitgestellte Schnittstelle testet.

\chapter{Datenmodell der Metadaten}
Für vereinfachte Such- und Sortieraufgaben wird bereits ein vereinfachtes Datenmodell festgelegt, welches
später auf Datenbank und XML-Daten umgesetzt werden soll: \\\\
\begin{tabular}{|l|l|}
	\hline
	Name & Datentyp \\
	\hline
	URL & String \\
	Dateipfad des Archivordners im Archiv & String \\
	Datum der letzten Änderung & TimeStamp oder Integer \\
	Commit Tag der Versionsverwaltung & String \\
	\hline
\end{tabular}

\chapter{Anforderungen}

\section{Crawlermodul}
\subsection{Crawlersteuerung} \label{l:spec:crawler:control}
	\subsubsection{Config-Datei} \label{l:spec:crawler:control:config} 
		Die Steuerung des bzw. der Crawler erfolgt über eine Config-Datei.
		Darin werden bereits Default- und Fallbackwerte festgelegt  
		Es können folgende Parameter eingestellt werden:
		\begin{enumerate}
			\item Tiefe bis zu der Links gefolgt werden soll
			\item Zeitintervalle der Crawlvorgänge
			\item maximale Anzahl der gleichzeitig gestarteten Crawlerinstanzen
			\item Filtereinstellungen
		\end{enumerate}

	\subsubsection{Kommandozeileninterface} \label{l:spec:crawler:control:cmd} 
		Optional kann das Modul mittels Kommandozeileninterface mit Parametern gestartet werden.
		Damit kann man z.B. Werte aus der Config-Datei überschreiben.
		Außerdem kann man Domains als Liste angeben, die als Startpunkte für die Crawler verwendet werden sollen. 
 
\subsection{Crawlerinstanziierung} \label{l:spec:crawler:instance}
	Pro Domain wird eine Crawlerinstanz gestartet bis die Obergrenze an Instanzen erreicht wird.
	Für die Crawlerinstanzen wird ein externes Tool verwendet (z.B. wget).
	Jede gestartete Instanz kopiert den Inhalt der Seite in je ein temporäres Verzeichnis.
	Dabei wird die online vorhandene URL-Pfadstruktur der HTML-Dateien auf das Dateisystem abgebildet.
	Je Domain wird dadurch ein Hauptverzeichnis erzeugt.
\subsection{Bereinigung und Normalisierung} \label{l:spec:crawler:normalize}
	Nach diesem Vorgang werden die temp-Ordner bereinigt (z.B. leere Ordner entfernt)
	und die HTML-Dateien in ein Archiv-Ordner gleichen Namens (inklusive Dateiendung) kopiert.
\subsection{Extraktion der Metadaten}\label{l:spec:crawler:extraction}
	Nun werden die Metadateien im XML-Format extrahiert und im o.g. Ordner gespeichert.
\subsection{Synchronisation}\label{l:spec:crawler:sync}
	Zuletzt werden die so vorbereiteten tmp-Ordner in das vorhandene Archiv hineinsynchronisiert (z.B. mit rsync). 
	Dabei wird jeder Domainordner über ein Dateimutex gelockt, um gleichzeitiges Schreiben zu verhindern. \\
	Zum Abschluss werden die Änderungen den Versionsverwaltungen der Domainordner mit einem Commit bestätigt.
	Während oder nach dem Synchronisationsvorgang wird ein Datenbank-Dump für die neuen oder geänderten Daten
	zur Aktualisierung der Datenbank erstellt.

\section{Archiv}
\subsection{Aufteilung:}
	Das Verzeichnis ist in einzelne Domainordner getrennt. 
\subsection{Versionierung:}
	Jeder Domainordner wird über ein Versionsverwaltung (z.B. git) versioniert. 
	Damit ist das Wiederherstellen älterer Versionen grundsätzlich möglich,
	wobei diese aber manuell über die git-Schnittstellen abgerufen werden müssen.
\subsection{Synchronisation}
	Bei Schreibvorgängen muss ähnlich wie unter \ref{l:spec:crawler:sync} beschrieben, 
	die Daten gegen konkurrierende Dateizugriffe gesichert werden.
	Dabei wird immer der ganze Domainordner gesperrt.
\subsection{Dateisystem:} 
	Beim darunterliegenden Dateisystem wird von einem vorhandenen Unix-FS ausgegangen.
\subsection{Komprimierung:} 
	Eine explizite Dateikomprimierung wird erstmal nicht vorgesehen, ist aber zum Teil schon durch die Versionierung gegeben.
	In der Regel werden alte Revisionen gepackt.

\section{Programmierschnittstelle - Java-Client}
\subsection{Datenbankabfragen}
	Über diesen Client können vorbereitete SQL-Statements an einen Java-Server geschickt werden. 
	Die SQL-Abfrage wird soweit vorbereitet, dass nur noch ein SQL-Bedingungsausdruck für die WHERE-Clause angegeben werden muss.
	Als return-Wert wird eine Liste von Metadatenobjekten zurückgegeben.
\subsection{Dateiabfrage}
	Über die Metadatenobjekte kann man sich über eine gesonderte Anfrage die vorhandenen Archivordner aus dem Archiv nachladen.
\subsection{Erweiterung	der XML-Dateien}
Auch eine Erweiterung der XML-Dateien um neue Tags bzw. die Abfrage von Tags soll über die 
Metadatenobjekte möglich sein.

\section{XML-Metadaten}
Die XML-Datei enthält grundsätzliche Metainformationen über eine archivierte HTML-Datei.
Die XML-Datei soll auch nachträglich über eine Programmierschnittstelle um weitere XML-tags erweiterbar sein.
Umgesetzt wird dies über die oben genannten Objekte der Java-Metadatenklasse, 
der bei Übergabe eines Namens an eine get-Methode ein passender XML-Node herausgesucht wird.
Mittels einer set-Methode, die Namen und Inhalt des Tags als Parameter erhält, können neue Tags hinzugefügt werden. 

\section{Datenbank}
Die Datenbank dient zur Speicherung der grundlegenden Metadaten und soll schnelle Suchanfragen zu ermöglichen.
Die Datenbank muss beim Fertigstellen des Crawlvorgangs auf den neuesten Stand gebracht werden.
Sollte die Datenbank beschädigt oder geändert werden, dann soll diese wieder aus den
XML-Metadaten rekonstruiert werden können.
Aus diesem Grund wird erstmal von einer sqlite-Datenbank ausgegangen, 
da diese relativ einfach als Datei erstellt und wieder gelöscht werden kann. 

\section{Anmerkung: Programmiersprachen}
Für die systemnahen Programmteile, wie die Steuerung der Crawler wird die Sprache Python in der Version 2 (2.7) verwendet,
da hier bereits gute Libraries für den Zugriff auf das Dateisystem und die Versionsverwaltung vorhanden sind.
Die mehr objektorientierten Teile und Schnittstellen nach außen werden in Java 1.7 umgesetzt.
\newpage 
% Diagramm

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{spezi/webarchiv_design.pdf}
	\caption{Diagramm: Grundlegendes Design}
\end{figure}
 
