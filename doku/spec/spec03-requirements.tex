\chapter{Anforderungen}
Im folgenden sind die Anforderungen an die Software spezifiziert. Soweit möglich wurden
die Anforderungen schon in einzelne Komponenten und Module gegliedert. Eine grobe Übersicht gibt auch Diagramm
im Abschnitt \ref{spec:dia:moduls}.

Alle Anforderungen werden mit einer Kennnummer in der Form 
\[ \text{\reqPrefix}<Gruppenprefix> . <Anforderungsnummer> [. <Unternummer> ]\text{\reqPostfix} \]
gekennzeichnet, damit diese später wieder referenziert werden können.

\section{Crawlermodul}
\initReqgrp{Cr}
Dieses Modul soll als eigenständiger Prozess laufen und in regelmäßigen Abständen Crawlvorgänge starten
und die Daten in das Archiv schreiben.

\subsection{Steuerung}
\begin{description}
        \label{spec:conf}
	\item [\req{Config-file}{conf}]
		Die Steuerung des bzw. der Crawler erfolgt über eine Config-Datei.
		Ist die Config-Datei nicht vorhanden oder kann sie nicht gelesen werden,
        so werden hart codierte Werte verwendet.
		Es können folgende Parameter eingestellt werden:
		\begin{description}
			\item [\subreq{Suchtiefe}{depth}]
				Suchtiefe bis zu der Links gefolgt werden soll.
			\item [\subreq{Crawlintervall}{tinterval}]
				Zeitintervalle zwischen den Crawlvorgängen.
			\item [\subreq{Maximale Instanzen}{maxInst}]
				maximale Anzahl der gleichzeitig gestarteten Crawlerinstanzen.
			\item [\subreq{Filtereinstellungen}{filter}]
				Liste von Verwendeten Ausschlussfiltern und deren Modulpfad.
			\item [\subreq{Berücksichtigung von robots.txt}{robotsTxt}]
				Man kann einstellen, ob robots.txt berücksichtigen werden soll.
			\item [\subreq{User Agent}{userAgent}]
				Man kann einen User Agent festlegen.
			\item [\subreq{Archiv root Pfad}{root}]
				Es wird der Wurzelpfad für die Archivordner festgelegt.
		\end{description}
	\item [\req{Kommandozeileninterface}{cmd}]
		Optional können beim Start mittels Kommandozeile zusätzliche Parameter übergeben werden.
		\begin{description}
			\item [\subreq{Überschreiben}{overwrite}]
				Werte aus dem Config-file werden damit überschrieben.
			\item [\subreq{URL-Liste}{urllist}]
				Es kann eine Liste von URLs übergeben werden, 
				die als Startpunkte für die Crawler verwendet werden sollen.
				Die Übergabe von mindestens einem Element ist aber immer notwendig.
			\item [\subreq{DB-Recovery erzwingen}{recover}]
				Es kann ein Datenbank-Recovery erzwungen werden. Siehe auch \ref{req:Db:recovery}
		\end{description}
\end{description}

\subsection{Ausführung des Crawlvorgangs} \label{spec:crawler:exec}
\begin{description}
	\item [\req{Start in Intervallen}{startInterval}]
		Der Crawlvorgang wird immer wieder nach einem gemäß \ref{req:conf:tinterval} festgelegtem
		Interval neu in Gang gesetzt. 
	\item [\req{Parallelität}{concurrent}]
		Die Ausführung der Crawlvorgänge soll parallel durchgeführt werden.
	\item [\req{URL-Queue}{urlqueue}]
		Alle aus \ref{req:Cr:cmd:urllist} werden in eine Warteschlange geschrieben.
	\item [\req{Instanziierung}{instance}]
				Für die Crawlerinstanzen wird ein externes Tool verwendet: WGET
		\begin{description}
			\item [\subreq{Temporäre Ordner anlegen}{createTmp}]
				Für jede Crawlerinstanz wird ein eigenes temporäres Verzeichnis zum Speichern
				der Downloads angelegt. Im folgenden mit TMP abgekürzt.
			\item [\subreq{Start}{start}]
				Es werden solange Crawlerinstanzen mit URLs als Startpunkt aus der URL-Queue erzeugt,
				bis diese leer ist. 
			\item [\subreq{maximale Instanzen}{maxInst}]
				Wird die Obergrenze an maximal erzeugbaren Instanzen erreicht 
				(siehe \ref{req:Cr:conf:maxInst}),
				wird immer solange mit dem Erzeugen neuer Instanzen gewartet,
				bis die nächste	Crawlerinstanz fertig ist.
			\item [\subreq{Domain-Überschneidungen}{domain-intersection}]
				Um Überschneidungen zu vermeiden, werden Domainnamen anderer Instanzen ausgeschlossen.
		\end{description}
	\item [\req{Crawlen}{crawl}]
		Jede gestartete Instanz beginnt nun den Crawlvorgang.
		\begin{description}
			\item [\subreq{Herunterladen}{download}]
				Jede Instanz kopiert die heruntergeladene Dateien 
				der Seite in ein temporäres Verzeichnis je Crawlerinstanz.
			\item [\subreq{Ordnerstruktur}{structure}]
				Dabei wird die online vorhandene URL-Pfadstruktur der Dateien 
				auf das Dateisystem abgebildet.
				Je Domain wird dadurch ein Hauptverzeichnis im TMP-Ordner erzeugt.
		\end{description}
	\item [\req{Bereinigung}{clean}]
		In diesem Teil werden die heruntergeladenen Ordner bereinigt,
		also leere Ordner gelöscht.
	\item [\req{Normalisierung}{normalize}]
		\begin{description}
			\item [\subreq{Datei umbenennen}{rename}]
				Das File wird in ''data.<Endung>'' umbenannt. 
			\item [\subreq{Archivordner erzeugen}{createArc}] 
				Es wird ein Archivordner (im Folgenden kurz \arc\ genannt) 
				mit dem Namen der Datei (inklusive Dateiendung) 
				erzeugt.
			\item [\subreq{Datei verschieben}{move}]
				Die data-Datei wird in das soeben erzeugte \arc\ verschoben.
		\end{description}
	\item [\req{Extraktion der Metadaten}{metaextract}]
		In diesem Vorgang werden die im Datenmodell (Abschnitt \ref{spec:model}) 
		definierten Metadaten 
		aus der Datei extrahiert und zwischengespeichert. 
		\begin{description}
			\item [\subreq{url}{url}]
				Die Original-URL wird aus dem aktuellen Ordnerpfad abgeleitet.
			\item [\subreq{title}{title}]
				Falls die Datei ein HTML ist, 
				muss der Titel aus dem Inhalt geparst werden.
			\item [\subreq{mimeType}{mimeType}]
				Der MIME-Typ der Datei wird mit einer geeigneten Maßnahme ermittelt.
			\item [\subreq{path}{path}]
				Es wird der aktuelle absolute Ordnerpfad auf das \arc gespeichert.
			\item [\subreq{domain}{domain}]
				Der Name der Domain wird aus der dem Domainordnernamen kopiert.
			\item [\subreq{createTime}{createTime}]
				Das Erzeugungsdatum der Datei wird ermittelt.
		\end{description}
	\item [\req{Filterung}{filter}]
		Für alle Dateien wird eine Liste von Filtern durchlaufen.
		Jeder Filter prüft, ob die Datei behalten oder verworfen werden soll.
		Verworfene Dateien werden sogleich gelöscht.
	\item [\req{commitTime speichern}{commitTime}]
		Der Startzeitpunkt wird sofort nach dem Beginn des Crawlvorgangs zwischengespeichert.
		Siehe auch Abschnitt \ref{spec:model}.
	\item [\req{Erzeugung von XML-Dateien}{xml}]
		Die zwischengespeicherten Metadaten werden in XML-Metadateien 
		(siehe auch Abschnitt \ref{spec:xml}) 
		geschrieben und jeweils im zugehörigen \arc\ als ''data.xml'' gespeichert. 
	\item [\req{Synchronisation}{sync}]
		Die vorbereiteten Daten in den TMP-Ordnern werden nun in das vorhandene Archiv synchronisiert 
		(mit RSYNC). (Siehe auch Abschnitt \ref{spec:filearchive})
		\begin{description}
			\item [\subreq{Update}{update}]
				Veraltete \arc s werden komplett durch die neuen ersetzt
				(und dadurch von der Versionierung in einen älteren Versionstand verschoben).
			\item [\subreq{Nicht vorhandene Ordner}{del}]
				\arc s, welche im TMP-Ordner aber nicht im Webarchiv vorhanden sind, werden
				komplett aus dem Archiv gelöscht (und landen ebenfalls in der Versionierung)
		\end{description}
	\item [\req{Datenbankaktualisierung}{dbupdate}]
		Die Datenbank wird nun mithilfe der zwischengespeicherten Metadaten aktualisiert.
\end{description}

%\begin{comment}
\section{Filtermodule}
\initReqgrp{Fi}
\begin{description}
	\item [\req{Schnittstelle}{interface}]
		Alle Filtermodule sollen eine vorgegebene Funktionsschnittstelle erfüllen, 
		welcher ein Metadatenobjekt übergeben werden kann und einen Wahrheitswert zurückgibt.
	\item [\req{Konfiguration}{conf}]
		Ein Filtermodul kann über eigenes Config-file verfügen, welches im Verzeichnis
		der anderen Config-dateien gespeichert werden soll.
	\item [\req{Dateiüberprüfung}{filechk}]
		Ein Filter soll ein File mittels der gegebenen Metadaten überprüfen und 
		einen Wahrheitswert zurückgeben,
		ob dieses behalten oder verworfen werden soll.
		Die Prüfmethode ist dabei vom einzelnen Zweck des Filters abhängig.
	\item [\req{Implementierung Testfilter: MIME-Type-filter}{testfilter}]
		übergebene Dateien werden anhand ihres MIME-Types geprüft, ob sie behalten oder verworfen werden sollen.
		\begin{description}
			\item [\subreq{Includes}{includes}]
				Es kann eine Liste von MIME-Types angegeben werden, die behalten werden sollen. 
			\item [\subreq{Excludes}{excludes}]
				Es kann eine Liste von MIME-Types angegeben werden, die verworfen werden sollen. 
			\item [\subreq{Pattern}{pattern}]
				Mit dem '*'-Zeichen können Pattern für includes und excludes gebildet werden. 
				So ist es z.B. möglich alle MIME-Types oder alle eines Primärtypen auszuwählen.
		\end{description}
\end{description}

\section{Dateiarchiv} \label{spec:filearchive}
Das Dateiarchiv stellt den zentralen Speicherort aller Quell-, Meta-XML und sonstiger
hinzugefügter Daten dar.
Die Dateistruktur wird bereits vom Crawlvorgang vorgegeben, wird hier aber nochmal kurz erläutert:
\begin{itemize}
	\item Auf der obersten Ebene stehen die Domain-Ordner.
	\item Darunter wird die von der URL abgebildeten Struktur
		innerhalb der Domain nachgebildet.
	\item Die einzelnen Dateien werden durch \arc-Ordner ersetzt, bzw. in diese verschoben.
	\item Jedes \arc\ enthält die Quell-Datei, 
		die aber in data.<Endung> unbenannt wurde.
		sowie die dazugehörige XML-Datei (data.xml). 
		Ein \arc\ kann aber auch weitere Dateien enthalten, welche nach dem
		Crawlen hinzugefügt werden können.
\end{itemize}
\initReqgrp{Ar}
\begin{description}
	\item [\req{Berechtigungen}{rights}]
		Generell dürfen keine Änderungen an bestehenden Dateien durchgeführt werden 
		(Ausname: siehe \ref{req:Xm:extense}).
		\begin{description}
			\item [\subreq{Externe Benutzer}{extern}]
				Von exterenen Nutzern (Java-Clients) dürfen nur Dateien in \emph{aktuelle} \arc
				hinzugefügt werden.
			\item [\subreq{Crawler}{crawler}]
				Crawler dürfen neue \arc-Ordner hinzufügen oder alte löschen bzw. ersetzen, 
				wobei immer das gesamte \arc\ ersetzt wird. 
		\end{description}
	\item [\req{Synchronisation}{sync}]
		Um gleichzeitige Zugriffe auf Dateien zu verhindern, 
		muss Synchronisiert werden.
		\begin{description}
			\item [\subreq{Sicherungsmechanismus}{lock}]
				Zum Schutz der kritischen Stellen wird mit Locks bzw. Mutexen gearbeitet.
			\item [\subreq{Umfang des Schutzes}{lockarea}]
				Es wird immer der gesamte Domain-Ordner gesperrt.
			\item [\subreq{Lesezugriffe}{read}]
				Es muss vor jedem Lesezugriff ein Lock gesetzt und
				sofort wieder entfernt werden, nachdem die Daten ausgelesen wurden.
			\item [\subreq{Schreibzugriffe}{write}]
				Es muss vor jedem Schreibvorgang ein Lock gesetzt und
				nach dem Beenden des Vorgangs wieder entfernt werden.
		\end{description}
	\item [\req{Dateisystem}{fs}]
		Beim darunterliegenden Dateisystem wird von einem vorhandenen Unix-Filesystem ausgegangen.
	\item [\req{Komprimierung}{compress}]
		Eine explizite Dateikomprimierung wird nicht vorgesehen, 
		ist aber zum Teil schon durch die Versionierung gegeben, 
		da alte Revisionen gepackt abgelegt werden.
\end{description}

\subsection{Versionierung}
\begin{description}
	\item [\req{Versionsverwaltungssystem}{versionsystem}]
		Es wird das Versionsverwaltungssystem GIT verwendet.
	\item [\req{Domainversionierung}{domainversion}]	
		Jeder Domain-Ordner wird über eine dezentrale Versionsverwaltung (git) separat versioniert. 
		Damit ist auch das Wiederherstellen älterer Versionen möglich.
	\item [\req{Hinzufügen}{add}]
		Beim Hinzufügen von neuen Dateien müssen diese der Versionsverwaltung bekanntgemacht werden (git add).
	\item [\req{Commit}{commit}]
		Änderungen müssen stets mit einem Commit bestätigt werden.
		Dabei wird zur Identifikation immer die in Abschnitt \ref{spec:model} 
		definierte commitTime verwendet.
		Dabei ist zu beachten:
		\begin{description}
			\item [Während des Crawlvorgangs]
				Das Erzeugen neuer commitTimes während des Crawlvorgangs ist 
				im Abschnitt \ref{spec:crawler:exec} beschrieben
			\item [\subreq{Nach dem Crawlen}{postCrawl}]
				Beim nachträglichen Hinzufügen von Dateien wird die alte commitTime wiederverwendet.
		\end{description}
	\item [\req{Ändern und Löschen}{update}]
		Beim Überschreiben und Löschen von Dateien müssen keine besonderen Vorkehrungen getroffen werden.
		Diese werden von der Versionierung in eine tiefere Versionsebene verschoben.
	\item [\req{Schreiben in veraltete Verzeichnisse}{writeToOldDirs}]
		Sollte in Ausnahmefällen in veraltete Verzeichnisse geschrieben werden, 
		zum Beispiel weil die Daten von den Crawlern während einer Analyse geändert wurden,
		dann werden die Daten verworfen und der Schreiber muss benachrichtigt werden.
		(z.B. mittels einer Exception)
\end{description}


\section{Programmierschnittstelle - Java-Client}\label{spec:client}
\initReqgrp{Cl}
	Diese Schnittstelle soll die Anbindung der Analysemethoden ermöglichen und 
	macht gleichzeitig einen Zugriff über das Netzwerk möglich.
\begin{description}
	\item [\req{Konfiguration}{conf}]
		Die URL und der Port des Servers ist bei der Initialisierung der Clients im Konstruktor zu übergeben.
	\item [\req{Client-API}{api}]
		Für Benutzer des Clients wird eine Programmierschnittstelle in Java zur Verfügung gestellt.
		Die API umfasst dabei folgende Schnittstellen:
		\begin{description}
			\item [\subreq{MetaData}{meta}]
				Grundlegende Methoden einer Metadatenklasse.
				Neben den Metadateninformationen soll diese Klasse auch als
				Schlüsselelement zum Zugriff auf die Archivordner und XML-Dateien dienen.
			\item [\subreq{WebarchiveClient}{client}]
				Zentrale Schnittstelle zum Zugriff auf das Webarchiv, Details siehe unten.
			\item [\subreq{Observer}{observer}]
				Implementierungen dieses Observers können sich beim Client anmelden, 
				um über Änderungen informiert zu werden. 
				Die Schnittstelle enthält eine Methode um Update-Informationen zu erhalten.
		\end{description}
	\item [\req{Registrierung am Server}{register}]
		Alle aktiven Java-Clients werden beim Server gespeichert, 
		um diese über Änderungen informieren zu können.
		Beim Abmelden oder Beenden muss ein Client aus dieser Registrierung gelöscht werden.
	\item [\req{Observerregistrierung}{observer}]
		Benutzer des Clients können sich mittels o.g. Schnittstelle beim Client als Observer an- und abmelden.
	\item [\req{Benachrichtigungen}{notifies}]
		Vom Server erhaltene Änderungen (in Form von commitTags) werden an die Observer weitergereicht.
	\item [\req{Datenbankabfragen}{dbquery}]
		Es sollen auch vorbereitete SQL-Statements an den Server geschickt werden können. 
		Die SQL-Abfrage wird soweit vorbereitet, 
		dass nur noch ein SQL-Bedingungsausdruck für die WHERE-Klausel angegeben werden muss.
		Optional soll auch eine ORDER-BY-Klausel im selben Stil angegeben werden können. 
		Als return-Wert wird eine Liste von Metadatenobjekten zurückgegeben.
	\item [\req{Datei Listing}{ls}]
		Mittels der Metadatenobjekte kann man sich über eine gesonderte Anfrage eine Auflistung
		über den Inhalt eines \arc-Ordners zurückgeben lassen.
	\item [\req{Datei Lesen}{readFile}]
		Mittels Metadatenobjekt und Dateipfadangabe wird ein Dateistream zum Lesen zurückgegeben.
	\item [\req{Datei Schreiben}{writeFile}]
		Mittels Metadatenobjekt und Dateipfadangabe wird ein Dateistream zum Schreiben zurückgegeben.
		Wie in \ref{req:Ar:rights} beschrieben, dürfen dabei keine Dateien überschrieben werden. 
	\item [\req{Auslesen von zusätzlichen Tags}{selectTag}]
		Durch Übergabe eines Metadata-objekts und eines Tagnamens an eine get-Methode 
		wird ein passender XML-Node herausgesucht.
	\item [\req{Erweiterung	der XML-Daten}{addTag}]
		Mittels einer set-Methode, die Namen und Inhalt des Tags als Parameter erhält, 
		können neue Tags zur XML-Datei hinzugefügt werden.
	\item [\req{Test Analysetool}{testanalyzer}]
		Zur Demonstrations- und Testzwecken der Java-Clientschnittstelle wird ein Analysetool erstellt, 
		welches die Wörter in HTML-Dateien zählt. 
		\begin{description}
			\item [\subreq{Speicherung als Textdatei}{txt}]
				Das Ergebnis wird als Textdatei im Archiv gespeichert.
			\item [\subreq{Speicherung im XML}{xml}]
				Das Ergebnis wird der Stamm-XML-Datei als zusätzliches Element hinzugefügt.
		\end{description}
	\end{description}

\section{Server} \label{spec:server}
\initReqgrp{Sv}
\begin{description}
	\item [\req{Konfiguration}{conf}]
		Es ist der Port des Servers in einer Config-Datei zu hinterlegen.
		Desweiteren werden darin auch alle Pfade zu externe Ressourcen
		hinterlegt.
	\item [\req{Client-Server Kommunikation}{comm}]
		Es muss ein Nachrichtensystem zwischen dem Client und dem Server implementiert werden.
		Über bestimmte Kennungen (z.B. über enums) ist der Inhalt einer Nachricht zu kennzeichnen.
		Dabei müssen folgende Informationen Ausgetauscht werden können:
		\begin{description}
			\item [\subreq{Austausch von Stream-Daten}{streams}]
				Zwischen C. und S. müssen Daten in Form von Streams verschickt werden können.
				Diese müssen in geeigneter Form bei der Übertragung gepuffert werden.
			\item [\subreq{Exceptions}{exceptions}]
				Vom Clientanwender verursachte Exceptions werden an diesen weitergeleitet und 
				im Client erneut geworfen.
			\item [\subreq{Datenbankabfragen}{dbquery}]
				Datenbankanfragen werden in Form von SQL vom Client geschickt und diesem
				in Form von Metadaten-Objekten beantwortet.
			\item [\subreq{Änderungen im Archiv}{changes}]
				Diese Nachrichtenform enthält Informationen für Client, welche Änderungen im Archiv betreffen.
		\end{description}
	\item [\req{Clienten registrieren}{register}]
		Der Server hält eine Liste von angemeldeten Java-Clienten und 
		verwaltet die Verbindungen der Clienten.
	\item [\req{Clienten entfernen}{rmClients}]
		Clients werden aus der Registrierung entfernt bei Verbindungsverlust oder 
		beim Beenden der Clients. 
	\item [\req{Update-Notifier}{notifier}]
		Hierfür speichert er sich den Zeitpunkt der letzten Update-Suche und 
		vergleicht ihn mit dem Datum der Datensätze in der Datenbank.
		Werden Änderungen gefunden, werden das Datum und der Commit-Tag zwischengepuffert.
		\begin{description}
			\item [\subreq{Konfiguration}{config}]
				Der Benachrichtigungsintervall muss in einem Configfile gespeichert werden.
			\item [\subreq{Thread}{thread}]
				Der Update-Notifier ist ein eigens laufender Thread des Servers. 
			\item [\subreq{Suchintervalle}{interval}]
				Der Notifier in dem vorgegebenen Intervall (z.B. stündlich) in der Datenbank,
				ob neue Commit-Tags vorhanden sind.
			\item [\subreq{Zeitpunkt der letzten Suche}{lastSearch}]
				Es wird immer der Zeitpunkt der letzten Suche gespeichert, um die 
				Datenbank auf aktuelle Daten zu prüfen.
			\item [\subreq{Clienten informieren}{notify}]
				Sollte die Suche Ergebnisse zutage gefördert haben,
				wird eine Liste mit den neuen commitTags an die registrierten Clients geschickt.
				Siehe auch \ref{req:Cl:notifies}
		\end{description}
\end{description}

\section{XML-Dateien} \label{spec:xml}
\initReqgrp{Xm}
\begin{description}
	\item [\req{Inhalt}{content}]
		Die XML-Dateien enthalten in ihrem Wurzelknoten eine Meta- und einen Datenknoten.
		\begin{description}
			\item [\subreq{Metaknoten}{meta}]
				Der Metaknoten ist für die in \ref{spec:model} beschriebenen Metadaten reserviert.
			\item [\subreq{Datenknoten}{data}]
				Der Datenknoten ist anfangs leer und kann von Benutzern um weitere Knoten erweitert werden.
		\end{description}
	\item [\req{Erstellung eines XML-Schemas}{schema}]
		Für die Validierung der XML-Daten muss eine XML-Schema ausgearbeitet werden.
	\item [\req{Erweiterbarkeit des Datenelements}{extense}]
		Wie oben beschrieben ist beim Design auf Erweiterbarkeit zu achten.
		\begin{description}
			\item [\subreq{Name eines Elements}{name}]
				Der Name eines hinzugefügten Elements ist frei wählbar, darf aber nur einmal
				auf der Ebene unter dem Datenknoten vorkommen.
			\item [\subreq{Inhalt}{content}]
				Struktur und Inhalt der hinzugefügten Elemente ist frei wählbar.
			\item [\subreq{Erweiterung des Schemas}{schema}]
				Bei Erweiterung des Datenknotens ist das Schema auch entsprechend zu erweitern.
		\end{description}
	\item [\req{Validierung}{validate}]
		Eine Validierung ist dann durchzuführen, nachdem eine XML-Datei erweitert worden ist.
		Dabei auftretende Fehler werden in eine Log-Datei geschrieben.
	\item [\req{Schreibschutz}{writeprotect}]
		Es dürfen nur neue Knoten hinzugefügt werden dürfen und
		\begin{description}
			\item [\subreq{Kein Überschreiben}{noOverwrite}]
				Es dürfen keine vorhandenen Elemente überschrieben oder geändert werden können.
			\item [\subreq{Metaknoten gesperrt}{meta}]
				Der Metaknoten darf nicht geändert werden.
		\end{description}
\end{description}
	
\section{Datenbank} \label{spec:db}
\initReqgrp{Db}
\begin{description}
	\item [\req{Konfiguration}{conf}]
		Die Datenbank wird als CREATE-TABLE-statement in einer SQL-Datei gespeichert.
	\item [\req{SQL in externen Dateien}{sqlFiles}]
		Vorbereitete SQL-Statements werden in je einer SQL-Datei in einem Ordner gespeichert.
		Inlinedefinitionen im Quellcode sind also zu vermeiden.
	\item [\req{Inhalt}{content}]
		Es werden die Speicherstände aller Dateien und Versionen im Archiv festgehalten.
        Die gespeicherten Daten entsprechen dem Datenmodell des Abschnitts \ref{spec:model}.
	\item [\req{Normalisierung}{normalize}]
		Um die Datenbank bei der großen Datenmenge klein zu halten soll die Datenbankstruktur
		weitestgehend normalisiert werden.
	\item [\req{Erweiterbarkeit}{extense}] 
		Eine Möglichkeit zum dynamischen Erweitern der Datenbank ist nicht vorgesehen.
	\item [\req{Berechtigungen}{rights}]
		\begin{description}
			\item [\subreq{Schreiben}{write}] Schreibrechte werden nur dem Crawlmodul erteilt.
			\item [\subreq{Lesen}{read}] Gelesen kann falls notwendig von allen Komponenten werden.
				Wobei externe Benutzer über die Schnittstellen der Clients SELECT-statements abzusetzen können.
		\end{description}
	\item [\req{Wiederherstellung}{recovery}]
		Sollte die Datenbank beschädigt oder geändert werden, dann soll diese wieder aus den
		im Archiv vorhanden XML-Metadaten aller Versionen rekonstruiert werden können.
\end{description}
%\end{comment}
