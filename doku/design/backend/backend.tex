\chapter{Backend}
\label{cha:backend}
Das Backend bezieht sich auf den in Python geschriebenen Teil, welcher öffentlich nicht sichtbar ist.
Im folgenden werden die zentralen Module aufgelistet, welche in eine Beschreibung ihrer Funktion und einer 
Schnittstellenbeschreibung aufgeteilt sind.

\section{Zentrale Module} 
\label{sec:zentrale_module}

\subsection{ConfigHandler}
\label{sub:confighandler}

\paragraph{Beschreibung:}
\label{par:beschreibung_}
Dieser wird zum Lesen und Setzen der Konfigurationswerte verwendet. Die Konfigurationsdatei ist in 
\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
Xml geschrieben und hat folgenden Aufbau:
    \lstinputlisting[language=XML,basicstyle=\ttfamily\fontsize{8}{10}\selectfont]{../../conf/webarchive.conf.xml}
Der Zugriff auf bestimmte Werte ist nach Domainprinzip aufgebaut, so lässt sich der Wert ,,Serverport'' folgendermaßen auslesen: \\
\texttt{webarchive.server.port} \\
Der erste Knoten \texttt{webarchive} ist stets vorhanden, und kann daher auch ausgelassen werden.


\subsection{Commandline Interface}
\label{sub:commandline_interface}
\paragraph{Beschreibung}
\label{par:beschreibung}
Dieses Interface realisiert die zentralle Administrationsschnittstelle zum Webarchiv. 
Das Interface soll dabei ähnlich wie git nach dem folgenden Schema funktionieren:
\begin{verbatim}
archive [--general-options] submodule [--arguments-specific-to-submodule]
\end{verbatim}
submodule kann dabei einer der folgenden Module sein:
\begin{table}[h]

\centering
\begin{tabular}{|l|l|}
    \hline
init & Initialisiert ein leeres archiv an einem bestimmten Pfad \\
    \hline
crawler & Zugriff auf Starten und Stoppen des Crawlvorgangs \\
    \hline
javadapter & Starten und Stoppen der Java Schnittstelle \\
    \hline
db & Bietet Funktionen um die Datenbank neu zu generieren \\
    \hline
config & Bietet Zugriff um Werte aus der config zu holen oder zu überschreiben \\
    \hline
\end{tabular}
\end{table}

\subsection{Initialisierung} 
\label{sub:initialisierung}
\paragraph{Beschreibung:}
\label{par:beschreibung}
% paragraph beschreibung (end)
Durch dieses Modul wird ein leerer Archivordner erstellt, der lediglich
ein Default-Configtemplate mit dem Pfad zum Archiv enthält, sowie einen leeren Ordner ,,content'',
in dem später die Crawldaten abgelegt werden.
Weitere Einstellung müssen händisch in der Config nachgetragen werden.
\\
Angelegt wird der Ordner über %\verbatim{archive init /pfad/zum/archive}
\paragraph{Schnittstelle:}
\label{par:schnittstelle}
\begin{lstlisting}[language=python]
def init\_archive(path = '.'):
    """
    Initializes empty archive structure with default Config
    at ,,path'' with the following structure:

      archive/
        content/ 
           # empty
        webarchive.config.xml
    """
    pass
\end{lstlisting}


% paragraph schnittstelle_ (end)
\subsection{Crawlermanager}
\label{sub:crawlermanager}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
% paragraph beschreibung (end)
Der Crawlermanager liest eine Liste mit URLs aus einer Datei, die entweder auf der Kommandozeile oder in
der Config definiert wurde. Die URLs sind in diese Datei zeilenweise einzutragen und können durch ein \# auskommentiert werden.
Über einen ThreadPool wird dann für jede URL ein Crawljob gestartet. Die maximale Anzahl der dabei laufenden Crawljobs wird bei
der Instanzierung des ThreadPools aus der Config gelesen.
\\
Desweiteren hat der Crawlermanager nur administrative Funktionen wie dem Stoppen der laufenden Crawljobs.
\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
Prinzipieller Aufbau des Crawlmanagers:
% paragraph schnittstelle_ (end)
\begin{lstlisting}[language=python]
class CrawlManager:
    def __init__(self):
        """
        Instance a new CrawlerManager,
        which must be started via start()
        """
        pass

    def start(self):
        """
        Reads configured URLs from Config, and start a new job for each
        """
        pass

    def shutdown(self,hard=False): 
        """
        Shuts down currently running crawljobs
        If hard is set to True, currently running jobs
        are canceled immediately, instead of syncing already
        gathered data to archive.
        """
        pass

    def reload(self):
        """
        Reloads configured URLs and replaces current Queue with new URLs.
        Leaves running Jobs untouched.
        """
        pass

\end{lstlisting}


% paragraph api_ (end)
\subsection{Crawljob}
\label{sub:crawljob}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Der Crawljob ist ein autonomer Thread der die unten aufgelisteten Submodule beeinhaltet.
Bevor die einzelnen Submodule abgearbeitet werden, wird ein temporäres Arbeitsverzeichnis angelegt.
Die Lage dieses Verzeichnisses kann in der Confug festgelegt werden.
\paragraph{API:}
\label{par:api_}

\begin{lstlisting}[language=python]
class CrawlJob():
    def __init__(self,ident,url):
        """
        Gets Identity and Url that should be crawled,
        Identity is needed to generate tmp folder
        """
        pass

    def shutdown(self,hard=False): 
        """
        Shuts down currently running crawljob
        If hard is set to True, currently running job
        is canceled immediately, instead of syncing already
        gathered data to archive.
        """
        pass
\end{lstlisting}

\subsubsection{Wget}
\label{ssub:wget}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
% paragraph beschreibung_ (end)
Eine Managementschicht zur Abstraktion von wget.
Dieser wird eine Domain zugeteilt welche von wget mit konfigurierbaren Parametern gecrawlt wird.
Die allgemeine Webseitenstruktur wird dabei intakt gelassen:

\begin{verbatim}
    www.heise.de/
        index.html
        news/
            newsfeeds.html
            favicon.png
        ...
\end{verbatim}

\paragraph{Schnittstelle:}
\label{par:schnittstelle_}

\begin{lstlisting}[language=python]
class Wget():
    def __init__(self, tmpfolder, url):
        """
        Instance a new Wget Wrapper,
        which must be started with start()

        :tmpfolder: The folder where the downloaded data will be stored
        :url: The url you want to be crawled recursively
        """
        pass

    def start(self): 
        """
        Start crawling the URL in the background,
        the function returns immediately
        """
        pass

    def stop(self):
        """
        Stop a running wget instance
        If start() was not called, or finished already,
        this function is a No-op
        """
        pass
\end{lstlisting}

% paragraph schnittstelle_ (end)
\subsubsection{Cleaner}
\label{ssub:cleaner}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Der Cleaner bereingt die Verzeichnisstruktur indem er leere Ordner und Dateien löscht.
Desweiteren wird folgende Restrukturierung durchgeführt:
\begin{verbatim}
    www.heise.de/ 
        index.html/
            data
        news/
            newsfeeds.html/
                data
            favicon.png/
                data
        ...
\end{verbatim}


\begin{itemize}
    \item Ordner werden intakt gelassen
    \item Für jede reguläre Datei wird ein gleichnamiges Verzeichnis angelegt
    \item Die regulären Daten werden in dieses Verzeichnis mit den Namen ,,data'' verschoben.
\end{itemize}

Außerdem werden während des Strukturierens die grundlegenden Metadatan als Liste im Speicher gesammelt.
Bevor die Datei an das Filtersystem weitergeleitet wird, wird ein ,,Titel'' abhängig vom MIME-Type ermittelt.
Pro Datei werden die konfigurierten Filter ausgeführt, welche entscheiden ob eine Datei gelöscht werden soll oder nicht.
Gibt ein Filter ,,false'' zurück, so wird der jeweilige Dateiordner samt Inhalt sowie die Metadatan gelöscht,
ohne dass weitere Filter aufgerufen werden müssen.
\\
Die gesammelten Metadaten werden in einem Metadatenobjekt gesammelt. Die gesammelten Daten entsprechen dabei 
der Liste aus \ref{spec:model} abzüglich der ,,commitTime'', da diese erst beim Check-In erhoben wird.

\paragraph{Schnittstelle:} 
\label{par:schnittstelle_}
\begin{lstlisting}[language=python]
class Cleaner():
    def __init__(self, path):
        """
        Instance a new cleaner, which is
        supposed to cleanup recursively in ,,path''
        """
        pass

    def restructure(self): 
        """
        Traverse the filetree and restructure
        it as described above.
        """
        pass
  
    def cleanup(self):
        """
        Remove emtpy files and directories
        in ,,path'' recursively
        """
        pass
\end{lstlisting}



% paragraph schnittstelle_ (end)
\subsubsection{Filtersystem}
\label{ssub:filtersystem}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Das Filtersystem entscheidet aufgrund eines übergebenen Metadaten-Dictionaries ob eine Datei behalten wird. Exemplarisch wird ein HTML-Filter
implementiert. Die Filter werden jeweils in einem Subintepreter gestartet, wobei die Metadaten im Subintepreter global vorhanden sind.

\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
Filter werden während des ,,Cleaner'' Vorgangs ausgeführt.
\begin{lstlisting}[language=python]
    # Inside the filter file two additional global variables
    # are defined:
    #     filter_input  : A copy of a metadata dictionary object
    #     filter_result : The decision of the filter is stored here
    #                     by default the decision is True.
    #
    # Note: filter *.py files cannot be run directly, since 
    #       filter_input and filter_result will not be defined there
    #
    # Example:
    if filter_input['mimeType'] == 'image/gif':
        filter_result = False
\end{lstlisting}
% paragraph schnittstelle_ (end)


\subsubsection{Xml Generator}
\label{ssub:xmlgen}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
% paragraph beschreibung_ (end)
Hierbei werden aus der Metadatenliste Xml-Dateien zum jeweiligen Content geschrieben (data.xml). 
Beim Xml generieren werden die Metadaten mit dem aktuellen Systemdatum versehen.
\paragraph{Schnittstelle:}
\label{par:schnittstelle_}

\begin{lstlisting}[language=python]
class XmlGenerator:
    def __init__(self, meta_obj):
        """
        Build a data.xml from a template and the meta_obj dictionary
        """
        pass

    def write(self, path = None):
        """
        Write XML to location ,,path'', 
        if not specified the path from meta_obj is taken

        Throws: IOError on failure
        """
        pass
\end{lstlisting}
% paragraph schnittstelle_ (end)

\subsubsection{DB Generator}
\label{ssub:dbgen}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Analog zur Xml-Generierung wird ein SQL-Statement erstellt, dass Daten aktualisiert oder neu hinzufügt.
Ist die Datenbank noch nicht vorhanden so wird sie neu erstellt.
% paragraph beschreibung_ (end)

\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
\begin{lstlisting}[language=python]
class DBGenerator:
    def __init__(self, meta_obj_list):
        """
        Build a SQL Query from the meta_obj_list to update and insert new items the Database 
        """
        pass

    def commit(self):
        """
        Send Query to Database,
        this function blocks till finished

        Throws: IOError on failure
        """
        pass
\end{lstlisting}



% paragraph schnittstelle_ (end)

\subsubsection{Rsync}
\label{ssub:rsync}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Rsync ist eine Managementschicht für das Unix Tool ,,rsync''. Rsync wird verwendet um die gecrawlten Daten sauber ins Archiv zu synchronisieren.
Hierbei wird der aktuelle gecrawlte Inhalt ins Archiv gespiegelt. 
% paragraph beschreibung_ (end)

\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
\begin{lstlisting}[language=python]
class Rsync:
    def __init__(self, path_src, path_dest):
        """
        Instance a new Rsync-Wrapper object,
        which will sync path_src to path_dest
        after calling start_sync() 
        """
        pass

    def start_sync(self):
        """
        Start the synchronisation process,
        this function will block until finished
        """
        pass
\end{lstlisting}

\subsubsection{Git}
\label{ssub:git}
\paragraph{Beschreibung:}
\label{par:beschreibung_}
Das Git Modul ist ein Wrapper für das SCM ,,git''. Es wird verwendet um das Archiv mit einer Versionierung auszustatten. Hierzu werden 
die Dateien ins Archiv per Git hinzugefügt, der Stand wird ,,commited'' und ein CommitTag generiert.

\paragraph{Schnittstelle:}
\label{par:schnittstelle_}
\begin{lstlisting}[language=python]
class Git:
    def __init__(self, domain, check_in_date):
        pass

    def checkout(self, tag = None):
        pass

    def commit_and_tag(self):
        pass
\end{lstlisting}

\subsection{Logger}
\paragraph{Beschreibung:}
\label{par:beschreibung}


\label{sub:logger}
Der Logger wird für das zentrale Loggen im Backend verwendet. Er implementiert die folgenden Error Level:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
    \hline
    \texttt{critical} & Schwere unerwartete Fehler \\
    \hline
    \texttt{error} & Fehler aufgrund von fehlerhaften Eingaben \\
    \hline
    \texttt{warning} & Unkritische Fehler \\
    \hline
    \texttt{info} & Allgemeine Statusinformationen \\
    \hline
    \texttt{debug} & Debug-Informationen für Entwickler \\
    \hline
\end{tabular} 
\end{table}
% paragraph beschreibung (end)


\paragraph{Schnittstelle:}
\label{par:schnittstelle_}



% paragraph schnittstelle_ (end)
Folgende Funktionen werden bereitgestellt:
\begin{lstlisting}[language=python]
def print(severity, *messages):
    """
    Print an Array of messages with a certain severity
    """
    pass

def verbosity(severity):
    """
    Set the verbosity of the program, 

    Passing a severity of 'debug' for example means
    that all messages are printed, while passing 'warning'
    would mean that only Warnings, Errors and Critical Errors 
    are printed.

    This has only effect on terminal output,
    all logmessages will be written to file anyway.
    """
    pass
\end{lstlisting}

\section{Database Recovery} 
\label{sec:database_recovery}
Das Recovery-Werkzeug traversiert über alle momentan vorhandenen Domains, und laufen durch die
jeweilige Git-History von hinten nach vorne. Dabei wird aus den XML Metadaten eine neue Datenbank gebildet.
% section database_recovery (end)

\section{Util} 
\label{sec:util}
Das Util Modul stellt verscheidene Funktionen die von mehreren Modulen genutzt werden bereit. Erwähnenswert ist hier der ,,Lockmechanismus''
auf Dateisystemebene um Daten beim Zugriff auf gemeinsame  Ressourcen zu schützen.

\section{Javadapter} 
\label{sec:javadapter}
Der ,,Javadapter'' stellt eine Schnittstelle zwischen Python und Java dar. Er ist als Daemon implementiert der über einen Socket über einen definierten Port
erreichbar ist. Interagiert werden kann mit dem Adapter über ein Textprotokoll. Der Client (in unserem Fall der Java-Server) kann bestimmte Kommandos
zeilenweise an den Server schicken, welcher dann mit ,,OK'' oder ,,ACK [Fehler]' antwortet.
\\
Folgende Kommandos sollen dabei implementiert werden:
\begin{itemize}
    \item \texttt{lock [domain]}
        Lockt eine bestimmte Domain mittels einen Lockfiles
    \item \texttt{unlock [domain]}
        Löscht ein durch \texttt{lock} erzeugtes Lockfile
\end{itemize}
